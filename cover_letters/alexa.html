<h1>Amazon Alexa Team</h1>
<p>I have made several Alexa skills, two of which are published in the store (US &amp; UK only)</p>
<h3>My Skills:</h3>
<ul>
<li>
<p><a href="https://github.com/PeterMitrano/collablab_status">CollabLab Status</a> <em>Published</em></p>
<p>This was my first published skill. I made it to check whether or not the Makerspace Club I run at my school is open, and check who currently signed in. It's in python, as are all of my skills.</p>
</li>
<li>
<p><a href="https://github.com/PeterMitrano/my_finder">My Finder</a> <em>Published?</em></p>
<p>My Finder is current under certification as of 9/18/16. The idea is to tell your alexa device where you put stuff, and it will remember forever. For instance, your unpacking into a new apartment and you know you'll forget where you put box of school supplies. Tell alexa! The main challenge for this skill was that people have many names for their things. If you say "I put my bike helmet on the black rack by my bed", you might then ask "where is my helmet?". You might even ask "where did I put my bike thingy?". To handle this, I use the wildly popular Wordnet corpus in combination with FuzzyWuzzy. This allows me to both do fuzzy string matching to fix translation errors, and basic NLP. This way we can know that "towel" and "blanket" are similiar enough. To make this approximation clear to the user, I modified the speech output. On a perfect match, it will say "your <item> has location <location>". However, if it matches "blue bookbag" to "blue backpack", it will say "I don't know where the blue bookbag is, but I know blue backpack has location <location>". This way the user knows it wasn't an exact match, and in case we got it wrong the user won't be as frustrated".</p>
</li>
<li>
<p><a href="https://github.com/PeterMitrano/my_desk">My Desk</a></p>
</li>
<li><a href="https://github.com/PeterMitrano/my_cookbook">My Cookbook</a></li>
</ul>
<h3>My Workflow for Alexa Projects:</h3>
<ul>
<li>Vim, with ycm for static analysis and completion, and CtrlP for fuzzy file finding</li>
<li>nose for unit testing, coverage for code coverage checking, all run on Travis</li>
<li>Git (obviously)</li>
<li>Python 2.7.11 (matches version on Lambda)</li>
<li>AWS CLI &amp; sketchy Makefiles for faster uploading</li>
</ul>
<h3>What I'd like to do at Amazon:</h3>
<p>As a developer, I've thought alot about the design decisions made by the Alexa team and how they impact what we can and cannot do. The one that has given me the most trouble though is as follows: If I am writing a skill, I am allowed to recieve any intent at any time. For instance, I may ask the user "do you want to create an event?" and they might say "Blue" to match some color intent I have. Of course, in this case, I must now handle the fact that the user has just spouted nonsene at me and gracefully proceed somehow. However, there are cases where ambiguity between intents could be better resolved if the skill was allow to present to the Alexa service which intents it "expected" the user to respond with. Similiar to how I imagine well defined slot types are better able to coerse speech into dates or places, we could coerse speech into intents that make more sense. For instance, one of my skills lets the user tell Alexa where they are putting some item, and ask Alexa later where they put it. For item and location, I have a "LocationIntent" with a custom slot type. When I ask "What's the item?" and they say "my backpack" I essentially get the raw text. What happens if the users says an item that is similiar to another intent I have? If they say the item is "backpack", but "backpack" is also a location for items, it might give the wrong kind of intent.</p>
<p>To put it simply, I would like to try to design an interface where skills include a list of "expected intents" in each "ask" response, and that is used as input to the voice model to make it more likely that intents are routed correctly. This is, of course, a trade-off between faithful text-to-speech, and giving the skill what it wants to hear. However, I think a balance can be made that will increase the overall success of voice interactions with custom skills.</p>